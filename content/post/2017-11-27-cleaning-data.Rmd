---
title: "Cleaning Data"
author: "Daniel Mikel"
date: "2017-11-27T21:13:14-05:00"
output: html_document
---

## California Wine

The first project I'm going to work on is with California Wine Grape Data. The data we'll start with is hosted by the [United States Department of Agriculture's Grape Crush Report](https://www.nass.usda.gov/Statistics_by_State/California/Publications/Grape_Crush/Reports/) which collects wine price, production, and acreage for California by wine variety disaggregated to either the Grape Price District (this can mean one county or several counties) or the individual grape purchasing contract. 

![California Grape Price Districts Image](/home/dan/Data/NASS/Grape_Crush/pictures/Screenshot from 2017-11-28 20-33-12.png)

This California Wine has some interesting potential for future projects. On a personal level it will be good to practice to work with a real data set, and it's not like wine is completely unrelatable (even if I'm more of a beer person). It also presents fodder for furthering the project, ideas like scraping winery and wine sales to analyze mark up prices, or modeling price and production based on rainfall and temperture are some feasible next steps I could take this data. Even working just with the price and production data from the Grape Crush Report will give me practice cleaning data, working with tidyr and dplyr to manipulate a large dataset, and using ggplot2 and shiny to visualize data. 

## First look at the data

Cleaning data feels like doing someone else's homework. Here are the packages I'll be working with:

```{r, message = FALSE}
library("tidyverse")
library("plyr")
library("stringr")
library("gdata")
```

I've loaded library("gdata") for the read.xls() function, library("tidyverse") mostly out of habit but also for dplyr joins, library("stringr") will be used later for manipulating character vectors, and libary("plyr") for some apply() functions that don't come with base R. I've saved all of the .xls files from [The USDA](https://www.nass.usda.gov/Statistics_by_State/California/Publications/Grape_Crush/Reports/) and put all of the section 8 files in their own folder. There are over 17,000 rows of data, so we'll just look at the first 15

```{r, echo = FALSE, results = 'asis', fig.width = 9}
gc2016 <- read.xls("/home/dan/Data/NASS/Grape_Crush/all_section_8/xls/gc2016.xls")
names(gc2016) <- c("District, Type, and Variety", "Base Price Per Ton", "NA", "Brix Factors", "Tons")

knitr::kable(gc2016[2:16,1:5])
```

A few things now come to mind:

* **Tidiness** Each row corresponds to single grape contract in a given district with the grapes variety and the contracts volume (in tons). For a human this data is pretty easy to interpret but for a vectorized program this data will take considerable work in order to manipulate efficiently. 

* **Years** Within each years data there is no indication of the time the data is from. Since the data was meant to be included within a booklet of data from the same year. We'll need to add the years to the datasets before we combine them with eachother.

* **The Variety Column** The first column has a lot of information in it, too much even! The wine district, the wine type, and the wine variety, and various sums are all within this column. We'll need to split the data include in this column out, as much of the information refers to later observations, and uses titles and whitespace within the column to give information about the data itself. We'll need to draw out the extra data into it's own columns in order to process this with R. 

Let's start by solving the second problem, adding a year column based on the file name (which included the year) and saved the results as a .csv in a different directory for later use.

```{r, eval=FALSE}
to_xls <- "/home/dan/Data/NASS/Grape_Crush/all_section_8/xls/"

files <- list.files(path = to_xls, full.names = TRUE)

Make_Year <- function(folder){
	files <- list.files(path = folder, full.names = TRUE)
	for(i in 1:length(files)){
		curr_year <- gsub(folder, "", files[i])
		curr_year <- gsub("/gc", "", curr_year)
		curr_year <- gsub(".xls", "", curr_year)
		data <- read.xls(files[i])
		names(data) <- c("Variety", "Base.Price.Per.Ton", "Trash", "Brix.Code", "Tons")
		data$Year <- curr_year
		write.csv(data, 
			paste0("/home/dan/Data/NASS/Grape_Crush/all_section_8/clean_csv/gc", curr_year, ".csv"),
			row.names = FALSE)
	}
}

Make_Year(to_xls)
```

## Building a dictionary for wine varieties and districts

At some point in the data cleaning process, we're going to need a way to parse the unneeded rows in the dataset from the rows with data we want to keep. We can either do this by:

* (1) excluding all the rows we don't need or 

* (2) finding a way to lift out all the rows we want. 

Since we're going to be processing multiple Section 8's over a long period of time from the dataset, and since we can't assume that this data has all been processed excactly the same way (the files could and probably were loaded in the year the report was published, giving 13 years for potential changes in processes). Some of the rows even include footnotes, and since going through 150,000 rows of data looking for each unique string of characters I don't want seems a little inefficient, I think (2) will be the safer assumption. 

Section 8 is what we want, but we'll start with another section to pull out a list of wine varieties included in the dataset. Section 2 is less than 200 rows long, and each unique variety has it's own row. 

```{r, echo = FALSE, results = 'asis', fig.width = 9}
sec2 <- read.csv("/home/dan/Data/NASS/Grape_Crush/all_section_2/2016 gcbtb02.csv")

knitr::kable(sec2[2:17,])
```

