---
title: "Scraping Presidential Approval - Part 2"
author: "Daniel Mikel"
date: "2017-12-16T14:27:12-05:00"
output: html_document
---

```{r, echo = FALSE, message = FALSE}
library("knitr")

tidy_pres <- read.csv("/home/dan/R/projects/get_election_data/presidential approval ratings/pres_approve_ucsb_prod/pres_approve_ucsb/tidy_pres.csv", 
                  stringsAsFactors =  FALSE)
```

The other day I built my first web scraper for a single page. I ended the blogpost pretty satisfied with myself and thought I'd come back to the project at a later date to try to build my first web scraper to take all the presidential approval ratings from the site. I didn't know how hard it would be at the time, and I was at the end of a long day at work. It might be because the University of California at Santa Barbara's [The American Presidency Project](http://www.presidency.ucsb.edu/index.php) was uniquely easy to scrape due to it's url but it took me like 7 minutes. A nice surprise. 

If want to read my previous post about scrapping, cleaning, and tidying data from a single page from the [The American Presidency Project](http://www.presidency.ucsb.edu/index.php) check out my previous post [here](https://abies.netlify.com/2017/12/scraping-presidential-approval/). 

I used two packages for this project: rvest for scrapping the data from the web, and tidyverse to do most of the manipulation of the scrapped data.

```{r, message = FALSE}
library("rvest")
library("tidyverse")
library("zoo")
library("scales")
```

Then I just used my function from last time, but just iterated over the different url's. I noticed that the two digit numeric chunk in the center of the url corresponed to the number of the presidency. Easy enough. All I needed was a column which I could use to discern different presidencys (I could go by data but this was more efficient). 

```{r, eval = FALSE}
all_pres <- as.data.frame(NULL)

for (i in 1:(45-31)){
  i <- i +31
  
  # get url
  url <- paste0("http://www.presidency.ucsb.edu/data/popularity.php?pres=", i, "&sort=time&direct=DESC&Submit=DISPLAY")
  
  # scrape data and save as character vector              
  webpage <- read_html(url)
  approval_html <- html_nodes(webpage, '.listdate')
  approval_data <- html_text(approval_html, trim = TRUE)
  
  # clean character vector of empty data
  approval_data[approval_data==""] <- NA
  approval_data[1:2] <- NA
  approval_data <- na.omit(approval_data)
  
  # convert to data.fame
  pres_matrix <- matrix(approval_data, ncol = 5, byrow = TRUE)
  pres_df <- as.data.frame(pres_matrix, stringsAsFactors = FALSE)
  names(pres_df) <- c("start_date", "end_date", "approve", "disapprove", "unsure")
  
  # clean up object classes
  pres_df[] <- lapply(pres_df, type.convert)
  pres_df$start_date <- as.Date(pres_df$start_date, 
                                 format = "%m/%d/%Y")
  pres_df$end_date <- as.Date(pres_df$end_date, 
                               format = "%m/%d/%Y")
  
  # keep information on which president the row of data represents
  pres_df$number <- i
  
  # get data into single dataframe
  all_pres <- rbind(all_pres, pres_df)
}
```

That was easier than I thought. I didn't really believe it would work on my first try. Scrapping data from mutliple webpages would probably be pretty challenging on a website with a more opaque url pattern, but I really shouldn't have psyched myself out of scrapping data from the web. 

On another note, I probably didn't need to clean the data that much before combining it into a single dataframe, but given the small volume of data I was processing this didn't noticably slow down my code. 

Now we just need to get it ready to graph:

```{r, eval = FALSE}
tidy_pres <- all_pres %>%
  gather(opinion, percent, -c(number, start_date, end_date)) 
```

Now we can graph whichever POTUS' approval rating we feel like.

```{r}

cols <- c("approve" = "#8BBF9F", "disapprove" = "#F7A278", "unsure" = "#A4BFEB")

tidy_pres %>%
  filter(number == "35") %>%
  ggplot(aes(x = end_date, y = percent, col = opinion)) +
  geom_jitter(alpha = 0.5, shape = 1) +
  geom_smooth(method = "gam",  formula = y ~ s(x, bs = "cs"), se = F) +
  scale_y_continuous(breaks = seq(0, 100, by = 10)) +
  theme_minimal() +
  theme(axis.text.x=element_text(angle=60, hjust=1)) + 
  scale_colour_manual(values = cols) + 
  ggtitle("Approval Rating: JFK", subtitle = NULL)
```

I made a shiny app:

```{r, echo = FALSE, fig.width = 8}

knitr::include_app("https://daniel-mikel.shinyapps.io/pres_approve_ucsb/", 
                   height = "650px")

```

For some reason running geom_smooth(method = "gam") was tripping an error when publishing on the shiny server. I ended up just switching to a loess function, but I wasn't happy that I couldn't find a direct fix. 

Here's the code for the shiny app:

```{r, eval = FALSE}
library(shiny)
library(tidyverse)
library(scales)

tidy_pres <- read.csv("tidy_pres.csv", stringsAsFactors = FALSE)

tidy_pres$start_date <- as.Date(tidy_pres$start_date, 
                                format = "%Y-%m-%d")
tidy_pres$end_date <- as.Date(tidy_pres$end_date, 
                              format = "%Y-%m-%d")
tidy_pres$number <- as.numeric(tidy_pres$number)

cols <- c("approve" = "#8BBF9F", "disapprove" = "#F7A278", "unsure" = "#A4BFEB")

ui <- fluidPage(
  
  titlePanel("Presidential Approval Rating"),
  
  fluidRow(
    column(selectizeInput("number", "Select POTUS",
                          choices = c("Franklin D. Roosevelt" = "32",
                                      "Harry S. Truman" = "33",
                                      "Dwight D. Eisenhower" = "34",
                                      "John F. Kennedy" = "35",
                                      "Lyndon B. Johnson" = "36",
                                      "Richard Nixon" = "37",
                                      "Gerald Ford" = "38",
                                      "Jimmy Carter" = "39",
                                      "Ronald Reagan" = "40",
                                      "George H. W. Bush" = "41",
                                      "Bill Clinton" = "42",
                                      "George W. Bush" = "43",
                                      "Barack Obama" = "44",
                                      "Donald Trump" = "45"),
                          selected = "35"),
           width = 3),
    
    plotOutput("pres_plot")
  )
)

server <- function(input, output){
  filtered <- reactive({
    tidy_pres %>%
      filter(number == input$number) 
  })
  
  output$pres_plot <- renderPlot({
    ggplot(data = filtered(), aes(x = as.Date(end_date, format = "%Y-%m-%d"), y = percent, col = opinion)) +
      geom_jitter(alpha = 0.6, size = 1.5) +
      stat_smooth(span = 0.3, se= F) +
      scale_x_date(date_breaks = "4 month", labels = date_format("%m-%Y")) +
      scale_y_continuous(breaks = seq(0, 100, by = 10)) +
      ylab("Percent") +
      xlab(NULL) +
      theme_minimal() +
      theme(axis.text.x=element_text(angle=60, hjust=1)) + 
      scale_colour_manual(values = cols)
    
  })
}

shinyApp(ui = ui, server = server)
```

The visual style is pretty similar to the (FiveThirtyEight's Trump Approval Tracker)[https://projects.fivethirtyeight.com/trump-approval-ratings/?ex_cid=rrpromo], just with less functionallity and data. I should really come up with a more distinct visual style...

I'll probably set this project down for a little while after this. I'm pretty happy that I've gotten some experience with web scrapping, and it's pretty cool to finally be using my R skills for something politics related. 
